# -*- coding: utf-8 -*-
"""nltk-tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19zZJl9LVXjiMFHrXdXoy_VVFrIwCmwbU
"""

!pip install nltk

"""# Tokenizing words and sentences """

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('punkt')

# tokenizing - word tokenizers ... sentence tokenizers
# lexicon and corporas
# corporea - body of text, ex: medical journals, presidential speeches, English language.
# lexicon - words and their means

text_sample = "Hello Mr. Johnson, how are you doing? The weather is great and python is awesom. The sky is pinkish blue."

print(sent_tokenize(text_sample))
print(word_tokenize(text_sample))

"""# Stop words"""

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

example_sentence = "This is an example showing off stop word filteration."
stop_words = set(stopwords.words("english"))
#print(stop_words)

words = word_tokenize(example_sentence)

filtered_sentence = [w for w in words if w not in stop_words]
print(filtered_sentence)

"""# Stemming"""

# reading, reads ==(stemmed to)==> read

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()

example_words = ["developer", "developing", "develops"]

for w in example_words: 
  print(ps.stem(w))

new_text = "You have to be the best developer to develop the apps; while developing take care of ;"

words = word_tokenize(new_text)

post_text = " ".join([ps.stem(w) for w in words])
print(post_text)

"""# Part of Speech"""

import nltk
nltk.download('tagsets')
nltk.download('state_union')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer

#help(state_union)
train_text = state_union.raw("/content/sample_data/joe-biden-march-2022")
sample_text = state_union.raw("/content/sample_data/joe-biden-sep-2022")

from google.colab import drive
drive.mount('/content/drive')

custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # train 

tokenized = custom_sent_tokenizer.tokenize(sample_text) #

def process_content():
   try: 
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)
      print(tagged)   
   except Exception as e:
     print(e)

process_content()

# TO KNOW THE ENGLISH AVAILABLE PART-OF-SPEECHES
nltk.help.upenn_tagset()



"""# Chunking"""

import nltk
nltk.download('state_union')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer

# https://www.nltk.org/book_1ed/ch07.html
#help(state_union)
train_text = state_union.raw("/content/sample_data/joe-biden-march-2022")
sample_text = state_union.raw("/content/sample_data/joe-biden-sep-2022")


custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # train 
tokenized = custom_sent_tokenizer.tokenize(sample_text) # 


def process_content():
   try: 
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      chunkGram = r"""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}"""
      chunkParser = nltk.RegexpParser(chunkGram)
      
      chunked = chunkParser.parse(tagged)
      #print(chunked)
   except Exception as e:
     print(e)

process_content()

"""# Chinking"""

# https://www.nltk.org/book_1ed/ch07.html
#help(state_union)
train_text = state_union.raw("/content/sample_data/joe-biden-march-2022")
sample_text = state_union.raw("/content/sample_data/joe-biden-sep-2022")


custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # train 
tokenized = custom_sent_tokenizer.tokenize(sample_text) # 


def process_content():
   try: 
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      chunkGram = r"""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}
                             }<VB.?|IN|DT|TO>+{"""
      chunkParser = nltk.RegexpParser(chunkGram)
      
      chunked = chunkParser.parse(tagged)
      #print(chunked)
   except Exception as e:
     print(e)

process_content()



"""# Named Entitiy Recogantion """

import nltk
nltk.download('state_union')
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
nltk.download('words')

from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer

# https://www.nltk.org/book/ch07.html#named-entity-recognition

"""
Commonly Used Types of Named Entity:

ORGANIZATION	Georgia-Pacific Corp., WHO
PERSON	Eddy Bonte, President Obama
LOCATION	Murray River, Mount Everest
DATE	June, 2008-06-29
TIME	two fifty a m, 1:30 p.m.
MONEY	175 million Canadian Dollars, GBP 10.40
PERCENT	twenty pct, 18.75 %
FACILITY	Washington Monument, Stonehenge
GPE	South East Asia, Midlothian
"""

# https://www.nltk.org/book_1ed/ch07.html
#help(state_union)
train_text = state_union.raw("/content/biden-may-2022.txt")
sample_text = state_union.raw("/content/biden-sep-2022.txt")


custom_sent_tokenizer = PunktSentenceTokenizer(train_text) # train 
tokenized = custom_sent_tokenizer.tokenize(sample_text) # 


def process_content():
   try: 
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      namedEnt = nltk.ne_chunk(tagged) # ,binary=True

      print(namedEnt)
   except Exception as e:
     print(e)

process_content()

"""# Lemmatizing

Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.
"""

from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

# default pos='n' = noun
# other options such, a=adj

print(lemmatizer.lemmatize('better'))
print(lemmatizer.lemmatize('better', pos="a"))
print(lemmatizer.lemmatize('best', pos="a"), '\n')
print(lemmatizer.lemmatize("corpora"))


print(lemmatizer.lemmatize('run'))
print(lemmatizer.lemmatize('run', pos="a"))

"""# NLTK Corpora"""

from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

import nltk
nltk.download('gutenberg')

print("nltk path:: ", nltk.__file__)

sample = gutenberg.raw('melville-moby_dick.txt')

tok = sent_tokenize(sample)

print(tok[1:5])



"""# WordNet"""

from nltk.corpus import wordnet

keyword = 'good'

syns = wordnet.synsets(keyword)

# sysnsel
print(syns[0].name)

#just the word
print(syns[0].lemmas()[0].name())

#defination
print(syns[0].definition())

#examples
print(syns[0].examples())


# synonyms and antonyms
synonyms = []
antonyms = []

for syn in wordnet.synsets(keyword):
  for l in syn.lemmas():
    synonyms.append(l.name())
    if l.antonyms():
      antonyms.append(l.antonyms()[0].name())

print(set(synonyms))
print(set(antonyms))


# check word similarity
w1 = wordnet.synset('ship.n.01')
w2 = wordnet.synset('boat.n.01')

print(w1.wup_similarity(w2))


w1 = wordnet.synset('ship.n.01')
w2 = wordnet.synset('cat.n.01')

print(w1.wup_similarity(w2))


w1 = wordnet.synset('ship.n.01')
w2 = wordnet.synset('car.n.01')

print(w1.wup_similarity(w2))

"""# Text Classifcation

"""

import nltk 
import random
from nltk.corpus import movie_reviews

nltk.download('movie_reviews')

documents = [(list(movie_reviews.words(fileid)), category)
            for category in movie_reviews.categories()
            for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

#print(documents[1])

all_words = []
for w in movie_reviews.words():
  all_words.append(w.lower())


all_words = nltk.FreqDist(all_words)

#print(all_words.most_common(15))
#print(all_words['stupid'])

# words as features for learning

word_features = list(all_words.keys())[:3000]

def find_features(document):
  words = set(document)
  features = {}
  for w in word_features:
    features[w] = w in words
  return features

print(find_features(movie_reviews.words('neg/cv000_29416.txt')))

featuresets = [(find_features(rev), category) for (rev, category) in documents]

#print(featuresets)


# NAIVE BAYES
training_set = featuresets[:1900]
testing_set = featuresets[1900:]

classifier = nltk.NaiveBayesClassifier.train(training_set)
print("Naive Bays Algo accuracy: ", (nltk.classify.accuracy(classifier, testing_set))*100)
print(classifier.show_most_informative_features(15))



import pickle

# export model using pickle

pickle_file = 'naivebayes.pickle'
# save_classifier = open(pickle_file,'wb')
#pickle.dump(classifier, save_classifier)
#save_classifier.close()

# import model from pickle
#classifier_f = open(pickle_file)
#classifier = pickle.load(classifier_f)
#classifier_f.close()



"""## Scikit-learn incorporation"""

from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC

from nltk.classify.scikitlearn import SklearnClassifier

MNB_classifier = SklearnClassifier(MultinomialNB())
MNB_classifier.train(training_set)
print("MNB_classifier accuracy percent: ", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)

#GNB_classifier = SklearnClassifier(GaussianNB())
#GNB_classifier.train(training_set)
#print("GNB_classifier accuracy percent: ", (nltk.classify.accuracy(GNB_classifier, testing_set))*100)

BNB_classifier = SklearnClassifier(BernoulliNB())
BNB_classifier.train(training_set)
print("BNB_classifier accuracy percent: ", (nltk.classify.accuracy(BNB_classifier, testing_set))*100)


#SVM 

SVC_classifier = SklearnClassifier(SVC())
SVC_classifier.train(training_set)
print("SVC_classifier accuracy percent: ", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)


LSVC_classifier = SklearnClassifier(LinearSVC())
LSVC_classifier.train(training_set)
print("LSVC_classifier accuracy percent: ", (nltk.classify.accuracy(LSVC_classifier, testing_set))*100)

NuSVC_classifier = SklearnClassifier(NuSVC())
NuSVC_classifier.train(training_set)
print("NuSVC_classifier accuracy percent: ", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)

# 
LR_classifier = SklearnClassifier(LogisticRegression())
LR_classifier.train(training_set)
print("LR_classifier accuracy percent: ", (nltk.classify.accuracy(LR_classifier, testing_set))*100)

SGD_classifier = SklearnClassifier(SGDClassifier())
SGD_classifier.train(training_set)
print("SGD_classifier accuracy percent: ", (nltk.classify.accuracy(SGD_classifier, testing_set))*100)

# Combining Algos with a Vote

from nltk.classify import ClassifierI
from statistics import mode

class VoteClassifier(ClassifierI):
    def __init__(self, *classifiers):
        self._classifiers = classifiers

    def classify(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)
        return mode(votes)

    def confidence(self, features):
        votes = []
        for c in self._classifiers:
            v = c.classify(features)
            votes.append(v)

        choice_votes = votes.count(mode(votes))
        conf = choice_votes / len(votes)
        return conf


voted_classifier = VoteClassifier(#classifier,
                                  MNB_classifier, BNB_classifier, 
                                  SVC_classifier, LSVC_classifier , NuSVC_classifier,
                                  LR_classifier, SGD_classifier)
print("voted_classifier accuracy percent:", (nltk.classify.accuracy(voted_classifier, testing_set))*100)

print("Classification: ", voted_classifier.classify(testing_set[0][0]), 
      "Confidence %:", voted_classifier.confidence(testing_set[0][0]))



